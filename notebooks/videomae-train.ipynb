{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c13404-1c69-4688-80d5-7aebc0a25829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/plzen1/home/strakajk/Projects/JSALT/MAE/videomae/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/auto/plzen1/home/strakajk/Projects/JSALT/MAE/videomae/modeling_finetune.py:298: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/auto/plzen1/home/strakajk/Projects/JSALT/MAE/videomae/modeling_finetune.py:307: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained=False, **kwargs):\n",
      "/auto/plzen1/home/strakajk/Projects/JSALT/MAE/videomae/modeling_finetune.py:316: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained=False, **kwargs):\n",
      "/auto/plzen1/home/strakajk/Projects/JSALT/MAE/videomae/modeling_finetune.py:325: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Namespace(batch_size=2, epochs=50, save_ckpt_freq=50, model='pretrain_videomae_base_patch16_224', decoder_depth=4, mask_type='tube', mask_ratio=0.9, input_size=224, drop_path=0.0, normlize_target=True, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.95], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.00015, warmup_lr=1e-06, min_lr=1e-05, warmup_epochs=40, warmup_steps=-1, use_checkpoint=False, color_jitter=0.0, train_interpolation='bicubic', data_path='../data/videos', imagenet_default_mean_and_std=True, num_frames=16, sampling_rate=2, output_dir='../output/videomae/19-05_19-14-48', log_dir=None, device='cuda', seed=0, resume='', auto_resume=True, start_epoch=0, num_workers=8, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', entity=None, project='', group=None, name='', tags=None, distributed=False)\n",
      "Creating model: pretrain_videomae_base_patch16_224\n",
      "{'pretrained_cfg': None, 'pretrained_cfg_overlay': None, 'drop_path_rate': 0.0, 'decoder_depth': 4, 'use_checkpoint': False}\n",
      "Patch size = (16, 16)\n",
      "Starting file check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 78.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clips: 4 -> 4\n",
      "Video duration: Min: 48 frames, Max: 381 frames, Mean: 155.250000 frames\n",
      "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x14da78f7d120>\n",
      "Model = PretrainVisionTransformer(\n",
      "  (encoder): PretrainVisionTransformerEncoder(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (decoder): PretrainVisionTransformerDecoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  )\n",
      "  (encoder_to_decoder): Linear(in_features=768, out_features=384, bias=False)\n",
      ")\n",
      "number of params: 94.210944 M\n",
      "LR = 0.00000117\n",
      "Batch size = 2\n",
      "Number of training steps = 2\n",
      "Number of training examples per epoch = 4\n",
      "Param groups = {\n",
      "  \"no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"mask_token\",\n",
      "      \"encoder.patch_embed.proj.bias\",\n",
      "      \"encoder.blocks.0.norm1.weight\",\n",
      "      \"encoder.blocks.0.norm1.bias\",\n",
      "      \"encoder.blocks.0.attn.q_bias\",\n",
      "      \"encoder.blocks.0.attn.v_bias\",\n",
      "      \"encoder.blocks.0.attn.proj.bias\",\n",
      "      \"encoder.blocks.0.norm2.weight\",\n",
      "      \"encoder.blocks.0.norm2.bias\",\n",
      "      \"encoder.blocks.0.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.0.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.1.norm1.weight\",\n",
      "      \"encoder.blocks.1.norm1.bias\",\n",
      "      \"encoder.blocks.1.attn.q_bias\",\n",
      "      \"encoder.blocks.1.attn.v_bias\",\n",
      "      \"encoder.blocks.1.attn.proj.bias\",\n",
      "      \"encoder.blocks.1.norm2.weight\",\n",
      "      \"encoder.blocks.1.norm2.bias\",\n",
      "      \"encoder.blocks.1.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.1.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.2.norm1.weight\",\n",
      "      \"encoder.blocks.2.norm1.bias\",\n",
      "      \"encoder.blocks.2.attn.q_bias\",\n",
      "      \"encoder.blocks.2.attn.v_bias\",\n",
      "      \"encoder.blocks.2.attn.proj.bias\",\n",
      "      \"encoder.blocks.2.norm2.weight\",\n",
      "      \"encoder.blocks.2.norm2.bias\",\n",
      "      \"encoder.blocks.2.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.2.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.3.norm1.weight\",\n",
      "      \"encoder.blocks.3.norm1.bias\",\n",
      "      \"encoder.blocks.3.attn.q_bias\",\n",
      "      \"encoder.blocks.3.attn.v_bias\",\n",
      "      \"encoder.blocks.3.attn.proj.bias\",\n",
      "      \"encoder.blocks.3.norm2.weight\",\n",
      "      \"encoder.blocks.3.norm2.bias\",\n",
      "      \"encoder.blocks.3.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.3.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.4.norm1.weight\",\n",
      "      \"encoder.blocks.4.norm1.bias\",\n",
      "      \"encoder.blocks.4.attn.q_bias\",\n",
      "      \"encoder.blocks.4.attn.v_bias\",\n",
      "      \"encoder.blocks.4.attn.proj.bias\",\n",
      "      \"encoder.blocks.4.norm2.weight\",\n",
      "      \"encoder.blocks.4.norm2.bias\",\n",
      "      \"encoder.blocks.4.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.4.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.5.norm1.weight\",\n",
      "      \"encoder.blocks.5.norm1.bias\",\n",
      "      \"encoder.blocks.5.attn.q_bias\",\n",
      "      \"encoder.blocks.5.attn.v_bias\",\n",
      "      \"encoder.blocks.5.attn.proj.bias\",\n",
      "      \"encoder.blocks.5.norm2.weight\",\n",
      "      \"encoder.blocks.5.norm2.bias\",\n",
      "      \"encoder.blocks.5.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.5.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.6.norm1.weight\",\n",
      "      \"encoder.blocks.6.norm1.bias\",\n",
      "      \"encoder.blocks.6.attn.q_bias\",\n",
      "      \"encoder.blocks.6.attn.v_bias\",\n",
      "      \"encoder.blocks.6.attn.proj.bias\",\n",
      "      \"encoder.blocks.6.norm2.weight\",\n",
      "      \"encoder.blocks.6.norm2.bias\",\n",
      "      \"encoder.blocks.6.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.6.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.7.norm1.weight\",\n",
      "      \"encoder.blocks.7.norm1.bias\",\n",
      "      \"encoder.blocks.7.attn.q_bias\",\n",
      "      \"encoder.blocks.7.attn.v_bias\",\n",
      "      \"encoder.blocks.7.attn.proj.bias\",\n",
      "      \"encoder.blocks.7.norm2.weight\",\n",
      "      \"encoder.blocks.7.norm2.bias\",\n",
      "      \"encoder.blocks.7.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.7.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.8.norm1.weight\",\n",
      "      \"encoder.blocks.8.norm1.bias\",\n",
      "      \"encoder.blocks.8.attn.q_bias\",\n",
      "      \"encoder.blocks.8.attn.v_bias\",\n",
      "      \"encoder.blocks.8.attn.proj.bias\",\n",
      "      \"encoder.blocks.8.norm2.weight\",\n",
      "      \"encoder.blocks.8.norm2.bias\",\n",
      "      \"encoder.blocks.8.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.8.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.9.norm1.weight\",\n",
      "      \"encoder.blocks.9.norm1.bias\",\n",
      "      \"encoder.blocks.9.attn.q_bias\",\n",
      "      \"encoder.blocks.9.attn.v_bias\",\n",
      "      \"encoder.blocks.9.attn.proj.bias\",\n",
      "      \"encoder.blocks.9.norm2.weight\",\n",
      "      \"encoder.blocks.9.norm2.bias\",\n",
      "      \"encoder.blocks.9.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.9.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.10.norm1.weight\",\n",
      "      \"encoder.blocks.10.norm1.bias\",\n",
      "      \"encoder.blocks.10.attn.q_bias\",\n",
      "      \"encoder.blocks.10.attn.v_bias\",\n",
      "      \"encoder.blocks.10.attn.proj.bias\",\n",
      "      \"encoder.blocks.10.norm2.weight\",\n",
      "      \"encoder.blocks.10.norm2.bias\",\n",
      "      \"encoder.blocks.10.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.10.mlp.fc2.bias\",\n",
      "      \"encoder.blocks.11.norm1.weight\",\n",
      "      \"encoder.blocks.11.norm1.bias\",\n",
      "      \"encoder.blocks.11.attn.q_bias\",\n",
      "      \"encoder.blocks.11.attn.v_bias\",\n",
      "      \"encoder.blocks.11.attn.proj.bias\",\n",
      "      \"encoder.blocks.11.norm2.weight\",\n",
      "      \"encoder.blocks.11.norm2.bias\",\n",
      "      \"encoder.blocks.11.mlp.fc1.bias\",\n",
      "      \"encoder.blocks.11.mlp.fc2.bias\",\n",
      "      \"encoder.norm.weight\",\n",
      "      \"encoder.norm.bias\",\n",
      "      \"decoder.blocks.0.norm1.weight\",\n",
      "      \"decoder.blocks.0.norm1.bias\",\n",
      "      \"decoder.blocks.0.attn.q_bias\",\n",
      "      \"decoder.blocks.0.attn.v_bias\",\n",
      "      \"decoder.blocks.0.attn.proj.bias\",\n",
      "      \"decoder.blocks.0.norm2.weight\",\n",
      "      \"decoder.blocks.0.norm2.bias\",\n",
      "      \"decoder.blocks.0.mlp.fc1.bias\",\n",
      "      \"decoder.blocks.0.mlp.fc2.bias\",\n",
      "      \"decoder.blocks.1.norm1.weight\",\n",
      "      \"decoder.blocks.1.norm1.bias\",\n",
      "      \"decoder.blocks.1.attn.q_bias\",\n",
      "      \"decoder.blocks.1.attn.v_bias\",\n",
      "      \"decoder.blocks.1.attn.proj.bias\",\n",
      "      \"decoder.blocks.1.norm2.weight\",\n",
      "      \"decoder.blocks.1.norm2.bias\",\n",
      "      \"decoder.blocks.1.mlp.fc1.bias\",\n",
      "      \"decoder.blocks.1.mlp.fc2.bias\",\n",
      "      \"decoder.blocks.2.norm1.weight\",\n",
      "      \"decoder.blocks.2.norm1.bias\",\n",
      "      \"decoder.blocks.2.attn.q_bias\",\n",
      "      \"decoder.blocks.2.attn.v_bias\",\n",
      "      \"decoder.blocks.2.attn.proj.bias\",\n",
      "      \"decoder.blocks.2.norm2.weight\",\n",
      "      \"decoder.blocks.2.norm2.bias\",\n",
      "      \"decoder.blocks.2.mlp.fc1.bias\",\n",
      "      \"decoder.blocks.2.mlp.fc2.bias\",\n",
      "      \"decoder.blocks.3.norm1.weight\",\n",
      "      \"decoder.blocks.3.norm1.bias\",\n",
      "      \"decoder.blocks.3.attn.q_bias\",\n",
      "      \"decoder.blocks.3.attn.v_bias\",\n",
      "      \"decoder.blocks.3.attn.proj.bias\",\n",
      "      \"decoder.blocks.3.norm2.weight\",\n",
      "      \"decoder.blocks.3.norm2.bias\",\n",
      "      \"decoder.blocks.3.mlp.fc1.bias\",\n",
      "      \"decoder.blocks.3.mlp.fc2.bias\",\n",
      "      \"decoder.norm.weight\",\n",
      "      \"decoder.norm.bias\",\n",
      "      \"decoder.head.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"encoder.patch_embed.proj.weight\",\n",
      "      \"encoder.blocks.0.attn.qkv.weight\",\n",
      "      \"encoder.blocks.0.attn.proj.weight\",\n",
      "      \"encoder.blocks.0.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.0.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.1.attn.qkv.weight\",\n",
      "      \"encoder.blocks.1.attn.proj.weight\",\n",
      "      \"encoder.blocks.1.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.1.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.2.attn.qkv.weight\",\n",
      "      \"encoder.blocks.2.attn.proj.weight\",\n",
      "      \"encoder.blocks.2.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.2.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.3.attn.qkv.weight\",\n",
      "      \"encoder.blocks.3.attn.proj.weight\",\n",
      "      \"encoder.blocks.3.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.3.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.4.attn.qkv.weight\",\n",
      "      \"encoder.blocks.4.attn.proj.weight\",\n",
      "      \"encoder.blocks.4.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.4.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.5.attn.qkv.weight\",\n",
      "      \"encoder.blocks.5.attn.proj.weight\",\n",
      "      \"encoder.blocks.5.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.5.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.6.attn.qkv.weight\",\n",
      "      \"encoder.blocks.6.attn.proj.weight\",\n",
      "      \"encoder.blocks.6.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.6.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.7.attn.qkv.weight\",\n",
      "      \"encoder.blocks.7.attn.proj.weight\",\n",
      "      \"encoder.blocks.7.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.7.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.8.attn.qkv.weight\",\n",
      "      \"encoder.blocks.8.attn.proj.weight\",\n",
      "      \"encoder.blocks.8.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.8.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.9.attn.qkv.weight\",\n",
      "      \"encoder.blocks.9.attn.proj.weight\",\n",
      "      \"encoder.blocks.9.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.9.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.10.attn.qkv.weight\",\n",
      "      \"encoder.blocks.10.attn.proj.weight\",\n",
      "      \"encoder.blocks.10.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.10.mlp.fc2.weight\",\n",
      "      \"encoder.blocks.11.attn.qkv.weight\",\n",
      "      \"encoder.blocks.11.attn.proj.weight\",\n",
      "      \"encoder.blocks.11.mlp.fc1.weight\",\n",
      "      \"encoder.blocks.11.mlp.fc2.weight\",\n",
      "      \"decoder.blocks.0.attn.qkv.weight\",\n",
      "      \"decoder.blocks.0.attn.proj.weight\",\n",
      "      \"decoder.blocks.0.mlp.fc1.weight\",\n",
      "      \"decoder.blocks.0.mlp.fc2.weight\",\n",
      "      \"decoder.blocks.1.attn.qkv.weight\",\n",
      "      \"decoder.blocks.1.attn.proj.weight\",\n",
      "      \"decoder.blocks.1.mlp.fc1.weight\",\n",
      "      \"decoder.blocks.1.mlp.fc2.weight\",\n",
      "      \"decoder.blocks.2.attn.qkv.weight\",\n",
      "      \"decoder.blocks.2.attn.proj.weight\",\n",
      "      \"decoder.blocks.2.mlp.fc1.weight\",\n",
      "      \"decoder.blocks.2.mlp.fc2.weight\",\n",
      "      \"decoder.blocks.3.attn.qkv.weight\",\n",
      "      \"decoder.blocks.3.attn.proj.weight\",\n",
      "      \"decoder.blocks.3.mlp.fc1.weight\",\n",
      "      \"decoder.blocks.3.mlp.fc2.weight\",\n",
      "      \"decoder.head.weight\",\n",
      "      \"encoder_to_decoder.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "optimizer settings: {'lr': 1.171875e-06, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.95]}\n",
      "Use step level LR & WD scheduler!\n",
      "Set warmup steps = 80\n",
      "Set warmup steps = 0\n",
      "Max WD = 0.0500000, Min WD = 0.0500000\n",
      "Auto resume checkpoint: \n",
      "Start training for 50 epochs\n",
      "Epoch: [0]  [0/2]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 1.4271 (1.4271)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8837 (0.8837)  time: 2.5145  data: 1.4003  max mem: 1939\n",
      "Epoch: [0]  [1/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4266 (1.4268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8794 (0.8816)  time: 1.3176  data: 0.7002  max mem: 2836\n",
      "Epoch: [0] Total time: 0:00:02 (1.3335 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4266 (1.4268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8794 (0.8816)\n",
      "Epoch: [1]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4279 (1.4279)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.9106 (0.9106)  time: 0.8516  data: 0.7262  max mem: 2836\n",
      "Epoch: [1]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4272 (1.4276)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8765 (0.8936)  time: 0.6390  data: 0.5184  max mem: 2836\n",
      "Epoch: [1] Total time: 0:00:01 (0.6620 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4272 (1.4276)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8765 (0.8936)\n",
      "Epoch: [2]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4260 (1.4260)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8853 (0.8853)  time: 1.1103  data: 0.9859  max mem: 2836\n",
      "Epoch: [2]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4260 (1.4267)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8735 (0.8794)  time: 0.7222  data: 0.6009  max mem: 2836\n",
      "Epoch: [2] Total time: 0:00:01 (0.7409 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4260 (1.4267)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8735 (0.8794)\n",
      "Epoch: [3]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4265 (1.4265)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8794 (0.8794)  time: 0.9593  data: 0.8345  max mem: 2836\n",
      "Epoch: [3]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4259 (1.4262)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8794 (0.8803)  time: 0.5385  data: 0.4173  max mem: 2836\n",
      "Epoch: [3] Total time: 0:00:01 (0.5599 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4259 (1.4262)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8794 (0.8803)\n",
      "Epoch: [4]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4253 (1.4253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8766 (0.8766)  time: 1.0804  data: 0.9567  max mem: 2836\n",
      "Epoch: [4]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4253 (1.4263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8766 (0.8850)  time: 0.5988  data: 0.4784  max mem: 2836\n",
      "Epoch: [4] Total time: 0:00:01 (0.6169 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4253 (1.4263)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8766 (0.8850)\n",
      "Epoch: [5]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4252 (1.4252)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8812 (0.8812)  time: 0.9605  data: 0.8283  max mem: 2836\n",
      "Epoch: [5]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4252 (1.4268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8812 (0.8887)  time: 0.6726  data: 0.5455  max mem: 2838\n",
      "Epoch: [5] Total time: 0:00:01 (0.6931 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4252 (1.4268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8812 (0.8887)\n",
      "Epoch: [6]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4253 (1.4253)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8768 (0.8768)  time: 1.0456  data: 0.9125  max mem: 2838\n",
      "Epoch: [6]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4253 (1.4258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8768 (0.8775)  time: 0.6346  data: 0.5083  max mem: 2838\n",
      "Epoch: [6] Total time: 0:00:01 (0.6548 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4253 (1.4258)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8768 (0.8775)\n",
      "Epoch: [7]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4231 (1.4231)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8770 (0.8770)  time: 0.9865  data: 0.8614  max mem: 2838\n",
      "Epoch: [7]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4231 (1.4254)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8770 (0.8874)  time: 0.6970  data: 0.5747  max mem: 2838\n",
      "Epoch: [7] Total time: 0:00:01 (0.7249 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4231 (1.4254)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8770 (0.8874)\n",
      "Epoch: [8]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4230 (1.4230)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8950 (0.8950)  time: 1.0592  data: 0.8960  max mem: 2838\n",
      "Epoch: [8]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4230 (1.4234)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8950 (0.8961)  time: 0.5996  data: 0.4527  max mem: 2838\n",
      "Epoch: [8] Total time: 0:00:01 (0.6340 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4230 (1.4234)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8950 (0.8961)\n",
      "Epoch: [9]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4227 (1.4227)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8913 (0.8913)  time: 0.9526  data: 0.8277  max mem: 2838\n",
      "Epoch: [9]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4227 (1.4236)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8887 (0.8900)  time: 0.5461  data: 0.4248  max mem: 2838\n",
      "Epoch: [9] Total time: 0:00:01 (0.5693 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4227 (1.4236)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8887 (0.8900)\n",
      "Epoch: [10]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4232 (1.4232)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8768 (0.8768)  time: 0.9273  data: 0.8029  max mem: 2838\n",
      "Epoch: [10]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4196 (1.4214)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8768 (0.8776)  time: 0.6353  data: 0.5146  max mem: 2838\n",
      "Epoch: [10] Total time: 0:00:01 (0.6574 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4196 (1.4214)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8768 (0.8776)\n",
      "Epoch: [11]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4229 (1.4229)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8946 (0.8946)  time: 1.0042  data: 0.8806  max mem: 2838\n",
      "Epoch: [11]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4206 (1.4218)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8792 (0.8869)  time: 0.5602  data: 0.4403  max mem: 2838\n",
      "Epoch: [11] Total time: 0:00:01 (0.5806 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4206 (1.4218)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8792 (0.8869)\n",
      "Epoch: [12]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4212 (1.4212)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8839 (0.8839)  time: 0.8320  data: 0.7064  max mem: 2838\n",
      "Epoch: [12]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4191 (1.4202)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8774 (0.8807)  time: 0.5227  data: 0.4014  max mem: 2838\n",
      "Epoch: [12] Total time: 0:00:01 (0.5418 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4191 (1.4202)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8774 (0.8807)\n",
      "Epoch: [13]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.4185 (1.4185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8817 (0.8817)  time: 0.9023  data: 0.7774  max mem: 2838\n",
      "Epoch: [13]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4184 (1.4185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8808 (0.8813)  time: 0.6542  data: 0.5329  max mem: 2838\n",
      "Epoch: [13] Total time: 0:00:01 (0.6758 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4184 (1.4185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8808 (0.8813)\n",
      "Epoch: [14]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4157 (1.4157)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8729 (0.8729)  time: 1.1054  data: 0.9825  max mem: 2838\n",
      "Epoch: [14]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4148 (1.4152)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8706 (0.8718)  time: 0.6119  data: 0.4913  max mem: 2838\n",
      "Epoch: [14] Total time: 0:00:01 (0.6353 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4148 (1.4152)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8706 (0.8718)\n",
      "Epoch: [15]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4151 (1.4151)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8615 (0.8615)  time: 1.0210  data: 0.8972  max mem: 2838\n",
      "Epoch: [15]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4151 (1.4168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8615 (0.8727)  time: 0.5684  data: 0.4486  max mem: 2838\n",
      "Epoch: [15] Total time: 0:00:01 (0.5878 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4151 (1.4168)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8615 (0.8727)\n",
      "Epoch: [16]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.4163 (1.4163)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8820 (0.8820)  time: 1.0761  data: 0.9490  max mem: 2838\n",
      "Epoch: [16]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4131 (1.4147)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8729 (0.8774)  time: 0.7340  data: 0.6124  max mem: 2838\n",
      "Epoch: [16] Total time: 0:00:01 (0.7543 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4131 (1.4147)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8729 (0.8774)\n",
      "Epoch: [17]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.4123 (1.4123)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8744 (0.8744)  time: 1.0791  data: 0.9493  max mem: 2838\n",
      "Epoch: [17]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.4123 (1.4125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8716 (0.8730)  time: 0.7088  data: 0.5839  max mem: 2838\n",
      "Epoch: [17] Total time: 0:00:01 (0.7291 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.4123 (1.4125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8716 (0.8730)\n",
      "Epoch: [18]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.4091 (1.4091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8557 (0.8557)  time: 1.0805  data: 0.9568  max mem: 2838\n",
      "Epoch: [18]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.4091 (1.4091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8557 (0.8594)  time: 0.5982  data: 0.4784  max mem: 2838\n",
      "Epoch: [18] Total time: 0:00:01 (0.6164 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.4091 (1.4091)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8557 (0.8594)\n",
      "Epoch: [19]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.4084 (1.4084)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8592 (0.8592)  time: 0.9916  data: 0.8672  max mem: 2838\n",
      "Epoch: [19]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.4084 (1.4085)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8592 (0.8610)  time: 0.6329  data: 0.5119  max mem: 2838\n",
      "Epoch: [19] Total time: 0:00:01 (0.6514 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.4084 (1.4085)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8592 (0.8610)\n",
      "Epoch: [20]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.4073 (1.4073)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8673 (0.8673)  time: 1.0555  data: 0.9318  max mem: 2838\n",
      "Epoch: [20]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.4069 (1.4071)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8634 (0.8654)  time: 0.5858  data: 0.4659  max mem: 2838\n",
      "Epoch: [20] Total time: 0:00:01 (0.6081 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.4069 (1.4071)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8634 (0.8654)\n",
      "Epoch: [21]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.4024 (1.4024)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8610 (0.8610)  time: 1.0797  data: 0.9557  max mem: 2838\n",
      "Epoch: [21]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.4024 (1.4026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8592 (0.8601)  time: 0.5985  data: 0.4779  max mem: 2838\n",
      "Epoch: [21] Total time: 0:00:01 (0.6210 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.4024 (1.4026)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8592 (0.8601)\n",
      "Epoch: [22]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.4013 (1.4013)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8490 (0.8490)  time: 1.0482  data: 0.9240  max mem: 2838\n",
      "Epoch: [22]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.4012 (1.4012)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8490 (0.8545)  time: 0.7174  data: 0.5966  max mem: 2838\n",
      "Epoch: [22] Total time: 0:00:01 (0.7374 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.4012 (1.4012)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8490 (0.8545)\n",
      "Epoch: [23]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3984 (1.3984)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8448 (0.8448)  time: 0.9744  data: 0.8435  max mem: 2838\n",
      "Epoch: [23]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3984 (1.3985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8429 (0.8439)  time: 0.5498  data: 0.4218  max mem: 2838\n",
      "Epoch: [23] Total time: 0:00:01 (0.5695 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3984 (1.3985)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8429 (0.8439)\n",
      "Epoch: [24]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3993 (1.3993)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8510 (0.8510)  time: 0.9018  data: 0.7761  max mem: 2838\n",
      "Epoch: [24]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3958 (1.3975)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8395 (0.8452)  time: 0.5096  data: 0.3881  max mem: 2838\n",
      "Epoch: [24] Total time: 0:00:01 (0.5295 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3958 (1.3975)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8395 (0.8452)\n",
      "Epoch: [25]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3947 (1.3947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8466 (0.8466)  time: 0.9447  data: 0.8209  max mem: 2838\n",
      "Epoch: [25]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3946 (1.3947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8466 (0.8507)  time: 0.5311  data: 0.4105  max mem: 2838\n",
      "Epoch: [25] Total time: 0:00:01 (0.5527 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3946 (1.3947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8466 (0.8507)\n",
      "Epoch: [26]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3927 (1.3927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8417 (0.8417)  time: 0.8880  data: 0.7634  max mem: 2838\n",
      "Epoch: [26]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3912 (1.3920)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8257 (0.8337)  time: 0.5273  data: 0.4062  max mem: 2838\n",
      "Epoch: [26] Total time: 0:00:01 (0.5496 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3912 (1.3920)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8257 (0.8337)\n",
      "Epoch: [27]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3919 (1.3919)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8322 (0.8322)  time: 0.8523  data: 0.7256  max mem: 2838\n",
      "Epoch: [27]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3909 (1.3914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8322 (0.8358)  time: 0.5709  data: 0.4492  max mem: 2838\n",
      "Epoch: [27] Total time: 0:00:01 (0.5893 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3909 (1.3914)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8322 (0.8358)\n",
      "Epoch: [28]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3887 (1.3887)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8345 (0.8345)  time: 0.9442  data: 0.8187  max mem: 2838\n",
      "Epoch: [28]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3871 (1.3879)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8345 (0.8371)  time: 0.5315  data: 0.4094  max mem: 2838\n",
      "Epoch: [28] Total time: 0:00:01 (0.5499 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3871 (1.3879)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8345 (0.8371)\n",
      "Epoch: [29]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.3850 (1.3850)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8393 (0.8393)  time: 1.0484  data: 0.9183  max mem: 2838\n",
      "Epoch: [29]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3850 (1.3855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8366 (0.8379)  time: 0.5843  data: 0.4592  max mem: 2838\n",
      "Epoch: [29] Total time: 0:00:01 (0.6044 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3850 (1.3855)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8366 (0.8379)\n",
      "Epoch: [30]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3814 (1.3814)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8232 (0.8232)  time: 0.9068  data: 0.7820  max mem: 2838\n",
      "Epoch: [30]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3814 (1.3820)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8227 (0.8229)  time: 0.5114  data: 0.3910  max mem: 2838\n",
      "Epoch: [30] Total time: 0:00:01 (0.5334 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3814 (1.3820)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8227 (0.8229)\n",
      "Epoch: [31]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3817 (1.3817)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8159 (0.8159)  time: 0.9779  data: 0.8517  max mem: 2838\n",
      "Epoch: [31]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3777 (1.3797)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7986 (0.8073)  time: 0.7286  data: 0.6069  max mem: 2838\n",
      "Epoch: [31] Total time: 0:00:01 (0.7490 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3777 (1.3797)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7986 (0.8073)\n",
      "Epoch: [32]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3769 (1.3769)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7998 (0.7998)  time: 0.9457  data: 0.8214  max mem: 2838\n",
      "Epoch: [32]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3758 (1.3763)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7977 (0.7988)  time: 0.6084  data: 0.4867  max mem: 2838\n",
      "Epoch: [32] Total time: 0:00:01 (0.6272 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3758 (1.3763)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7977 (0.7988)\n",
      "Epoch: [33]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3760 (1.3760)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8194 (0.8194)  time: 0.9419  data: 0.8185  max mem: 2838\n",
      "Epoch: [33]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3736 (1.3748)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8062 (0.8128)  time: 0.5297  data: 0.4093  max mem: 2838\n",
      "Epoch: [33] Total time: 0:00:01 (0.5498 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3736 (1.3748)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.8062 (0.8128)\n",
      "Epoch: [34]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3716 (1.3716)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7846 (0.7846)  time: 0.8612  data: 0.7363  max mem: 2838\n",
      "Epoch: [34]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3699 (1.3708)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7818 (0.7832)  time: 0.6188  data: 0.4984  max mem: 2838\n",
      "Epoch: [34] Total time: 0:00:01 (0.6388 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3699 (1.3708)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7818 (0.7832)\n",
      "Epoch: [35]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.3696 (1.3696)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7898 (0.7898)  time: 1.0467  data: 0.9230  max mem: 2838\n",
      "Epoch: [35]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3676 (1.3686)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7764 (0.7831)  time: 0.6451  data: 0.5248  max mem: 2838\n",
      "Epoch: [35] Total time: 0:00:01 (0.6654 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3676 (1.3686)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7764 (0.7831)\n",
      "Epoch: [36]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.3654 (1.3654)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7749 (0.7749)  time: 1.0449  data: 0.9213  max mem: 2838\n",
      "Epoch: [36]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3642 (1.3648)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7749 (0.7770)  time: 0.5812  data: 0.4607  max mem: 2838\n",
      "Epoch: [36] Total time: 0:00:01 (0.6041 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3642 (1.3648)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7749 (0.7770)\n",
      "Epoch: [37]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3642 (1.3642)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7720 (0.7720)  time: 0.8698  data: 0.7451  max mem: 2838\n",
      "Epoch: [37]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3603 (1.3623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7535 (0.7628)  time: 0.4937  data: 0.3726  max mem: 2838\n",
      "Epoch: [37] Total time: 0:00:01 (0.5118 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3603 (1.3623)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7535 (0.7628)\n",
      "Epoch: [38]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3596 (1.3596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7569 (0.7569)  time: 0.9335  data: 0.8079  max mem: 2838\n",
      "Epoch: [38]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3573 (1.3584)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7569 (0.7601)  time: 0.5260  data: 0.4040  max mem: 2838\n",
      "Epoch: [38] Total time: 0:00:01 (0.5455 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3573 (1.3584)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7569 (0.7601)\n",
      "Epoch: [39]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3560 (1.3560)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7434 (0.7434)  time: 0.8644  data: 0.7404  max mem: 2838\n",
      "Epoch: [39]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3529 (1.3545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7351 (0.7392)  time: 0.6076  data: 0.4870  max mem: 2838\n",
      "Epoch: [39] Total time: 0:00:01 (0.6292 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3529 (1.3545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7351 (0.7392)\n",
      "Epoch: [40]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3530 (1.3530)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7410 (0.7410)  time: 0.8734  data: 0.7492  max mem: 2838\n",
      "Epoch: [40]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3529 (1.3529)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7348 (0.7379)  time: 0.5502  data: 0.4292  max mem: 2838\n",
      "Epoch: [40] Total time: 0:00:01 (0.5734 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3529 (1.3529)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7348 (0.7379)\n",
      "Epoch: [41]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3506 (1.3506)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7355 (0.7355)  time: 0.8874  data: 0.7629  max mem: 2838\n",
      "Epoch: [41]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3472 (1.3489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7177 (0.7266)  time: 0.6197  data: 0.4994  max mem: 2838\n",
      "Epoch: [41] Total time: 0:00:01 (0.6385 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3472 (1.3489)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7177 (0.7266)\n",
      "Epoch: [42]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3469 (1.3469)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7185 (0.7185)  time: 0.9325  data: 0.8069  max mem: 2838\n",
      "Epoch: [42]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3469 (1.3473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7185 (0.7216)  time: 0.5251  data: 0.4035  max mem: 2838\n",
      "Epoch: [42] Total time: 0:00:01 (0.5433 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3469 (1.3473)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7185 (0.7216)\n",
      "Epoch: [43]  [0/2]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000001  loss: 1.3454 (1.3454)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7199 (0.7199)  time: 1.0956  data: 0.9715  max mem: 2838\n",
      "Epoch: [43]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3424 (1.3439)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7166 (0.7183)  time: 0.6065  data: 0.4858  max mem: 2838\n",
      "Epoch: [43] Total time: 0:00:01 (0.6245 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3424 (1.3439)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7166 (0.7183)\n",
      "Epoch: [44]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3428 (1.3428)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7227 (0.7227)  time: 0.8849  data: 0.7594  max mem: 2838\n",
      "Epoch: [44]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3405 (1.3416)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7041 (0.7134)  time: 0.5007  data: 0.3798  max mem: 2838\n",
      "Epoch: [44] Total time: 0:00:01 (0.5226 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3405 (1.3416)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7041 (0.7134)\n",
      "Epoch: [45]  [0/2]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.3399 (1.3399)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7028 (0.7028)  time: 0.8415  data: 0.7173  max mem: 2838\n",
      "Epoch: [45]  [1/2]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.3399 (1.3404)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7011 (0.7020)  time: 0.7067  data: 0.5859  max mem: 2838\n",
      "Epoch: [45] Total time: 0:00:01 (0.7257 s / it)\n",
      "Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.3399 (1.3404)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7011 (0.7020)\n",
      "Epoch: [46]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.3364 (1.3364)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6894 (0.6894)  time: 1.0918  data: 0.9686  max mem: 2838\n",
      "Epoch: [46]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3364 (1.3375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6880 (0.6887)  time: 0.6039  data: 0.4843  max mem: 2838\n",
      "Epoch: [46] Total time: 0:00:01 (0.6235 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3364 (1.3375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6880 (0.6887)\n",
      "Epoch: [47]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.3401 (1.3401)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.7138 (0.7138)  time: 0.9097  data: 0.7859  max mem: 2838\n",
      "Epoch: [47]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3367 (1.3384)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6885 (0.7012)  time: 0.5764  data: 0.4558  max mem: 2838\n",
      "Epoch: [47] Total time: 0:00:01 (0.5946 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3367 (1.3384)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6885 (0.7012)\n",
      "Epoch: [48]  [0/2]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.3381 (1.3381)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6941 (0.6941)  time: 1.0139  data: 0.8903  max mem: 2838\n",
      "Epoch: [48]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3369 (1.3375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6941 (0.6991)  time: 0.5655  data: 0.4452  max mem: 2838\n",
      "Epoch: [48] Total time: 0:00:01 (0.5856 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3369 (1.3375)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6941 (0.6991)\n",
      "Epoch: [49]  [0/2]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.3386 (1.3386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6922 (0.6922)  time: 0.9366  data: 0.8134  max mem: 2838\n",
      "Epoch: [49]  [1/2]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3368 (1.3377)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6872 (0.6897)  time: 0.5270  data: 0.4067  max mem: 2838\n",
      "Epoch: [49] Total time: 0:00:01 (0.5452 s / it)\n",
      "Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3368 (1.3377)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 0.6872 (0.6897)\n",
      "Training time 0:01:14\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "JOB_DIR=\"../output/videomae\"\n",
    "DATA_DIR=\"../data/videos\"\n",
    "\n",
    "#\"../data/videos\" \\\n",
    "python ../videomae/run_mae_pretraining.py \\\n",
    "    --output_dir ${JOB_DIR} \\\n",
    "    --data_path ${DATA_DIR} \\\n",
    "    --mask_type tube \\\n",
    "    --mask_ratio 0.9 \\\n",
    "    --model pretrain_videomae_base_patch16_224 \\\n",
    "    --decoder_depth 4 \\\n",
    "    --batch_size 2 \\\n",
    "    --num_frames 16 \\\n",
    "    --sampling_rate 2 \\\n",
    "    --opt adamw \\\n",
    "    --opt_betas 0.9 0.95 \\\n",
    "    --warmup_epochs 40 \\\n",
    "    --save_ckpt_freq 50 \\\n",
    "    --epochs 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea45dbb-43c2-4216-bb3b-03c1a5ed41e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
